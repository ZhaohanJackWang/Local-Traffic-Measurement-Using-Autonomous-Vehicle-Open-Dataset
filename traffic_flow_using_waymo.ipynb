{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python376jvsc74a57bd01c5cd709499cbb1ca066488b7e4f937f3d6c0854de00ce145ab3c2c144bd959a",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Import and Install Extensions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cloning into 'waymo-od'...\n",
      "remote: Enumerating objects: 1206, done.\u001b[K\n",
      "remote: Counting objects: 100% (322/322), done.\u001b[K\n",
      "remote: Compressing objects: 100% (207/207), done.\u001b[K\n",
      "remote: Total 1206 (delta 176), reused 220 (delta 104), pack-reused 884\u001b[K\n",
      "Receiving objects: 100% (1206/1206), 25.57 MiB | 9.51 MiB/s, done.\n",
      "Resolving deltas: 100% (756/756), done.\n",
      "* \u001b[32mmaster\u001b[m\n",
      "  \u001b[31mremotes/origin/HEAD\u001b[m -> origin/master\n",
      "  \u001b[31mremotes/origin/master\u001b[m\n",
      "  \u001b[31mremotes/origin/om2\u001b[m\n",
      "  \u001b[31mremotes/origin/r1.0\u001b[m\n",
      "  \u001b[31mremotes/origin/r1.0-tf1.15\u001b[m\n",
      "  \u001b[31mremotes/origin/r1.0-tf2.0\u001b[m\n",
      "  \u001b[31mremotes/origin/r1.2\u001b[m\n",
      "Note: checking out 'remotes/origin/r1.0'.\n",
      "\n",
      "You are in 'detached HEAD' state. You can look around, make experimental\n",
      "changes and commit them, and you can discard any commits you make in this\n",
      "state without impacting any branches by performing another checkout.\n",
      "\n",
      "If you want to create a new branch to retain commits you create, you may\n",
      "do so (now or later) by using -b with the checkout command again. Example:\n",
      "\n",
      "  git checkout -b <new-branch-name>\n",
      "\n",
      "HEAD is now at a66eb0a... Merge branch 'master' into r1.0\n"
     ]
    }
   ],
   "source": [
    "!rm -rf waymo-od > /dev/null\n",
    "!git clone https://github.com/waymo-research/waymo-open-dataset.git waymo-od\n",
    "!cd waymo-od && git branch -a\n",
    "!cd waymo-od && git checkout remotes/origin/r1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade pip\n",
    "#!pip install waymo-open-dataset\n",
    "#!pip install pandas\n",
    "#!pip install matplotlib\n",
    "#!pip install nltk\n",
    "#!pip install pytest\n",
    "#!pip install tensorflow\n",
    "#!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from waymo_open_dataset.utils import range_image_utils\n",
    "from waymo_open_dataset.utils import transform_utils\n",
    "from waymo_open_dataset.utils import frame_utils\n",
    "from waymo_open_dataset import dataset_pb2 as open_dataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'figure.max_open_warning': 0}) # Ignore warning about 20+ figures consuming memory\n",
    "import matplotlib.patches as patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/wang_ja/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "#import nltk\n",
    "#import ssl\n",
    "\n",
    "#try:\n",
    "#    _create_unverified_https_context = ssl._create_unverified_context\n",
    "#except AttributeError:\n",
    "#    pass\n",
    "#else:\n",
    "#    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "#nltk.download()"
   ]
  },
  {
   "source": [
    "## Sorting Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorting_data(frame_data, frame_num):\n",
    "    nltk_tokens = nltk.word_tokenize(str(frame)) # Tokenize frame data\n",
    "    df = pd.DataFrame() # Creating an empty dataframe\n",
    "    for i in range(len(nltk_tokens)): # For i in list indexes\n",
    "        if nltk_tokens[i] == \"laser_labels\": # Assign elements named \"laser_labels\" with index i\n",
    "            laser_data = [[ nltk_tokens[i], nltk_tokens[i+47], nltk_tokens[i+6], nltk_tokens[i+9], nltk_tokens[i+12], \\\n",
    "                nltk_tokens[i+15], nltk_tokens[i+18], nltk_tokens[i+21], nltk_tokens[i+24], nltk_tokens[i+30],\\\n",
    "                nltk_tokens[i+33], nltk_tokens[i+36], nltk_tokens[i+39], nltk_tokens[i+43] ]]\n",
    "            df = df.append(laser_data, ignore_index=True) # append the above parameters to empty df\n",
    "    df.columns = ['device','iD', 'center_x', 'center_y', 'center_z', 'width', 'length', 'height', \\\n",
    "                'heading', 'speed_x', 'speed_y', 'accel_x', 'accel_y', 'type']; # these are parameters taken from nltk_tokens\n",
    "\n",
    "    df = df[df['type'] != 'TYPE_PEDESTRIAN'] #ignore pedestiran data\n",
    "\n",
    "    df[['center_x', 'center_y', 'center_z', 'width', 'length', 'height', 'heading', 'speed_x', 'speed_y', 'accel_x', 'accel_y']] = df[['center_x', 'center_y', 'center_z', 'width', 'length', 'height', 'heading', 'speed_x', 'speed_y', 'accel_x', 'accel_y']].astype('float64');\n",
    "\n",
    "    # Convert DataFrame to .xslx\n",
    "    frame_xlsx ='frame' + str(frame_num) + '.xlsx'\n",
    "    df.to_excel(frame_xlsx)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "source": [
    "## Show Camera Image"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_camera_image(camera_image, camera_labels, layout, cmap=None):\n",
    "  \"\"\"Show a camera image and the given camera labels\"\"\"\n",
    "\n",
    "  ax = plt.subplot(*layout)\n",
    "\n",
    "  # Draw the camera labels\n",
    "  for camera_labels in frame.camera_labels:\n",
    "    # Ignore camera labels that do not correspond to this camera\n",
    "    if camera_labels.name != camera_image.name:\n",
    "      continue\n",
    "\n",
    "    # Iterate over the individual labels\n",
    "    for label in camera_labels.labels:\n",
    "      # Draw the object bounding box\n",
    "      ax.add_patch(patches.Rectangle(\n",
    "        xy=(label.box.center_x - 0.5 * label.box.length,\n",
    "            label.box.center_y - 0.5 * label.box.width),\n",
    "        width=label.box.length,\n",
    "        height=label.box.width,\n",
    "        linewidth=1,\n",
    "        edgecolor='red',\n",
    "        facecolor='none'))\n",
    "\n",
    "  # Show/download camera image\n",
    "  plt.imshow(tf.image.decode_jpeg(camera_image.image), cmap=cmap)\n",
    "  plt.title(open_dataset.CameraName.Name.Name(camera_image.name))\n",
    "  plt.grid(False)\n",
    "  plt.axis('off')\n",
    "  frame_img = 'frame' + str(i) + '.png'\n",
    "  plt.savefig(frame_img)"
   ]
  },
  {
   "source": [
    "## Finding AV speed and distance travelled"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_AV_speed_dis(df1, df2, frame_num):\n",
    "    sign_in1 = df1[df1['type'] == 'TYPE_SIGN'] # filter out rows with type 'TYPE_SIGN' from df1\n",
    "    sign_in2 = df2[df2['type'] == 'TYPE_SIGN'] # filter out rows with type 'TYPE_SIGN' from df2\n",
    "\n",
    "    for i in range(len(sign_in1.iloc[:, 1])): # for i in iD column in sign_in1\n",
    "        for j in range(len(sign_in2.iloc[:, 1])): # for j in iD column in sign_in2\n",
    "            if sign_in1.iloc[i, 1] == sign_in2.iloc[j, 1]: # if the signs have the same iD\n",
    "                AV_x_travelled = sign_in1.iloc[i, 2] - sign_in2.iloc[j, 2] # sign x coordinate difference \n",
    "                AV_y_travelled = sign_in1.iloc[i, 3] - sign_in2.iloc[j, 3] # sign y coordinate difference \n",
    "\n",
    "                AV_x_speed = AV_x_travelled/0.1 # AV x distance travelled/0.1s between frames\n",
    "                AV_y_speed = AV_y_travelled/0.1 # AV y distance travelled/0.1s between frames\n",
    "\n",
    "    dis = -math.sqrt(AV_x_travelled**2 + AV_y_travelled**2)\n",
    "    \n",
    "    return dis, AV_x_speed, AV_y_speed"
   ]
  },
  {
   "source": [
    "## Finding change in AV heading"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_AV_heading(df1, df2, frame_num):\n",
    "    sign_in1 = df1[df1['type'] == 'TYPE_SIGN'] # filter out rows with type 'TYPE_SIGN' from df1\n",
    "    sign_in2 = df2[df2['type'] == 'TYPE_SIGN'] # filter out rows with type 'TYPE_SIGN' from df2\n",
    "    \n",
    "    for i in range(len(sign_in1.iloc[:, 1])): # for i in iD column in sign_in1\n",
    "        for j in range(len(sign_in2.iloc[:, 1])): # for j in iD column in sign_in2\n",
    "            if sign_in1.iloc[i, 1] == sign_in2.iloc[j, 1]: # if the signs have the same iD\n",
    "                heading1 = sign_in1.iloc[i, 8]\n",
    "                heading2 = sign_in2.iloc[j, 8]\n",
    "    \n",
    "    AV_heading_change = heading1 - heading2 # sign heading difference \n",
    "                \n",
    "    return AV_heading_change"
   ]
  },
  {
   "source": [
    "## Finding Traffic Flow"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_traffic_flow(df1, df2, frame_num, AV_cum_dist, dis, AV_x_speed, AV_y_speed, cum_heading, AV_heading_change, final_data):\n",
    "    AV_row = {'device':'laser_labels', 'iD':'av', 'center_x':0, 'center_y':0, 'center_z':0, 'width':2.03, 'length':5.18, 'height':1.78, 'heading':0, 'speed_x':AV_x_speed, 'speed_y':AV_y_speed, 'accel_x':np.NaN, 'accel_y':np.NaN, 'type':'TYPE_VEHICLE'}\n",
    "    veh_in2 = df2[df2['type'] == 'TYPE_VEHICLE']\n",
    "    veh_in2 = veh_in2.append(AV_row, ignore_index=True)\n",
    "    veh_in2['center_x'] = veh_in2['center_x']\n",
    "    veh_in2 = veh_in2[abs(veh_in2['heading']) <= math.pi/4]\n",
    "    veh_in2 = veh_in2.sort_values(by=['center_x'])\n",
    "\n",
    "    # spacing and density:\n",
    "    list_1 = []\n",
    "    for i in range(len(veh_in2.iloc[:, 2])-1):\n",
    "        spacing = abs(veh_in2.iloc[i, 2]-veh_in2.iloc[i+1, 2])\n",
    "        list_1.append(spacing)\n",
    "    avg_spacing = sum(list_1)/(len(veh_in2.iloc[:, 2])-1)\n",
    "    density = 1/avg_spacing # k = 1/s\n",
    "\n",
    "    # space mean speed:\n",
    "    list_2 = []\n",
    "    for i in range(len(veh_in2.iloc[:, 9])):\n",
    "        veh_x_speed = veh_in2.iloc[i, 9]\n",
    "        veh_y_speed = veh_in2.iloc[i, 10]\n",
    "        veh_speed = math.sqrt(veh_x_speed**2 + veh_y_speed**2)\n",
    "        list_2.append(veh_speed)\n",
    "    avg_veh_speed = sum(list_2)/(len(veh_in2.iloc[:, 9]))\n",
    "\n",
    "    # flow:\n",
    "    flow = density*avg_veh_speed # q = k * v\n",
    "\n",
    "    # cumulative distance:\n",
    "    dis_travelled = {'AV Cumulative Distance (m)': [dis]}\n",
    "    cum_dist_df = pd.DataFrame(dis_travelled, columns = ['AV Cumulative Distance (m)'])\n",
    "    AV_cum_dist = AV_cum_dist.append(cum_dist_df) \n",
    "\n",
    "    # AV speed:\n",
    "    AV_speed = math.sqrt(AV_x_speed**2 + AV_y_speed**2) # change later\n",
    "\n",
    "    # AV heading using initial heading as reference zero:\n",
    "    heading_travelled = {'AV Cumulative Heading': [AV_heading_change]}\n",
    "    cum_heading_df = pd.DataFrame(heading_travelled, columns = ['AV Cumulative Heading'])\n",
    "    AV_cum_heading = cum_heading.append(cum_heading_df) \n",
    "\n",
    "    # add measurements to a df:\n",
    "    frame_rate = 10 # Hertz\n",
    "    measures = {'Frame': [frame_num], \\\n",
    "                'Time (s)': [(frame_num-1)/frame_rate], \\\n",
    "                'Flow (veh/h)':[flow], \\\n",
    "                'Density (veh/km)':[density], \\\n",
    "                'Spacing (km)':[avg_spacing], \\\n",
    "                'Space mean speed (km/hr)':[avg_veh_speed], \\\n",
    "                'AV distance travelled (km)':[AV_cum_dist.sum().sum()], \\\n",
    "                'AV heading (degrees)':[(cum_heading.sum().sum())*180/3.1415926536], \\\n",
    "                'AV speed (km/hr)':[AV_speed]}\n",
    "    traffic_measure = pd.DataFrame(measures, columns = ['Frame', \\\n",
    "                                                        'Time (s)', \\\n",
    "                                                        'Flow (veh/h)', \\\n",
    "                                                        'Density (veh/km)', \\\n",
    "                                                        'Spacing (km)', \\\n",
    "                                                        'Space mean speed (km/hr)', \\\n",
    "                                                        'AV distance travelled (km)', \\\n",
    "                                                        'AV heading (degrees)', \\\n",
    "                                                        'AV speed (km/hr)'])\n",
    "    final_data = final_data.append(traffic_measure, ignore_index = True)\n",
    "\n",
    "    return final_data, AV_cum_dist, AV_cum_heading"
   ]
  },
  {
   "source": [
    "## Iterate over all frames"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-13c8e2afed9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mshow_camera_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcamera_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi2\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# For 1st frame in pair (where index is odd), generate DF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m       \u001b[0mdf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorting_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mi2\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# For 2nd frame in pair (where index is even), generate DF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m       \u001b[0mdf2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorting_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-ff7136ced0d4>\u001b[0m in \u001b[0;36msorting_data\u001b[0;34m(frame_data, frame_num)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msorting_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mnltk_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Tokenize frame data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Creating an empty dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnltk_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# For i in list indexes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnltk_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"laser_labels\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Assign elements named \"laser_labels\" with index i\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \"\"\"\n\u001b[0;32m--> 144\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m     return [\n\u001b[1;32m    146\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \"\"\"\n\u001b[1;32m    105\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1275\u001b[0m         \u001b[0mGiven\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m         \"\"\"\n\u001b[0;32m-> 1277\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdebug_decisions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36msentences_from_text\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1329\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m         \"\"\"\n\u001b[0;32m-> 1331\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1329\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m         \"\"\"\n\u001b[0;32m-> 1331\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mspan_tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1319\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m             \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1322\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_realign_boundaries\u001b[0;34m(self, text, slices)\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \"\"\"\n\u001b[1;32m   1361\u001b[0m         \u001b[0mrealign\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1362\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1363\u001b[0m             \u001b[0msl1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrealign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msl2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_pair_iter\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    319\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_slices_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m         \u001b[0mlast_break\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1335\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperiod_context_re\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1336\u001b[0m             \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after_tok'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_contains_sentbreak\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "j = 1 # Start frame; min = 1\n",
    "k = 200 # End frame; max = 200\n",
    "FILENAME = '/Users/wang_ja/Desktop/waymo/training_training_0005/t_segment-12974838039736660070_4586_990_4606_990_with_camera_labels.tfrecord'\n",
    "\n",
    "i = 0 # Frame count; initialise as 0\n",
    "i2 = 1 # Frame pair count; initialise as 1\n",
    "\n",
    "final_data_append = pd.DataFrame()\n",
    "AV_cum_dist_append = pd.DataFrame()\n",
    "AV_cum_heading_append = pd.DataFrame()\n",
    "\n",
    "dataset = tf.data.TFRecordDataset(FILENAME, compression_type='')\n",
    "for data in dataset:\n",
    "\n",
    "  i += 1 # Count number of frames\n",
    "\n",
    "  if i<=j-1:\n",
    "    continue\n",
    "  frame = open_dataset.Frame()\n",
    "  frame.ParseFromString(bytearray(data.numpy()))\n",
    "  plt.figure(figsize=(25, 20))\n",
    "\n",
    "  for index, image in enumerate(frame.images):\n",
    "    show_camera_image(image, frame.camera_labels, [3, 3, index+1])\n",
    "    if i2%2 != 0: # For 1st frame in pair (where index is odd), generate DF\n",
    "      df1 = sorting_data(frame, i)\n",
    "    elif i2%2 == 0: # For 2nd frame in pair (where index is even), generate DF\n",
    "      df2 = sorting_data(frame, i)\n",
    "      if (index % 5) == 0: # if there are five images\n",
    "        AV_x_speed, AV_y_speed, dis = find_AV_speed_dis(df1, df2, i) # three variables from find_AV_speed_dis\n",
    "        AV_heading_change = find_AV_heading(df1, df2, i) # one varibale from find_AV_heading\n",
    "        final_data_append, AV_cum_dist_append, AV_cum_heading_append = find_traffic_flow(df1, df2, i, AV_cum_dist_append, dis, AV_x_speed, AV_y_speed, AV_cum_heading_append, AV_heading_change, final_data_append)\n",
    "        # df1, df2, frame_num, AV_cum_dist, dis, AV_x_speed, AV_y_speed, cum_heading, AV_heading_change, final_data\n",
    "        final_data_xlsx ='final_data' + str(i) + '.xlsx'\n",
    "        final_data_append.to_excel(final_data_xlsx, sheet_name='Raw')\n",
    "      else:\n",
    "        pass\n",
    "      df1=df2 # Assign 1st frame as 2nd frame\n",
    "      i2 += 1\n",
    " \n",
    "  i2 += 1\n",
    "\n",
    "  if i==k: \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}